{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae9ca24-dbd4-411c-b7b8-3d4ad09c4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from zipfile import ZipFile\n",
    "\n",
    "# with ZipFile('compressed.zip', 'r') as zf:\n",
    "#     zf.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6882b964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchinfo\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "103f9ac0-715c-416f-9cb1-62524c4d1ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms\n",
    "\n",
    "from models.Discriminator import *\n",
    "from models.Encoder import *\n",
    "from models.Generator import *\n",
    "from loss_functions import *\n",
    "from DAMSM_trainer import *\n",
    "\n",
    "import dataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import config.settings as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa3b7b1-4952-44b6-86f2-2c2fdc1a27d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.926204204559326\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "cur = time.time()\n",
    "\n",
    "imsize = 299\n",
    "transform=transforms.Compose([\n",
    "        transforms.Resize(int(imsize)),\n",
    "        transforms.RandomCrop(imsize),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "])\n",
    "\n",
    "data_set = dataset.TextDataset(os.getcwd(), transform=transform)\n",
    "\n",
    "print(time.time() - cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ee1b50-4413-4ea4-8472-62c61bc35f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.orthogonal_(m.weight.data, 1.0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.orthogonal_(m.weight.data, 1.0)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e2692a4-97ef-4b04-ac89-bfb8b7f4b618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwan/.local/lib/python3.10/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n",
      "/home/hwan/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "cnn_model = CNN_Encoder(config.EMBEDDING_DIM).to(config.DEVICE)\n",
    "rnn_model = RNN_Encoder(config.WORD_SIZE, number_hidden=config.EMBEDDING_DIM).to(config.DEVICE)\n",
    "\n",
    "rnn_model.load_state_dict(torch.load(\"saved_models/rnn_model_state_dict.pt\"))\n",
    "cnn_model.load_state_dict(torch.load(\"saved_models/cnn_model_state_dict.pt\"))\n",
    "\n",
    "cnn_model = cnn_model.to(config.DEVICE)\n",
    "rnn_model = rnn_model.to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4595bd-4c56-4f62-8329-7d8d95b36dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminators = [DiscriminatorNetwork().to(config.DEVICE), DiscriminatorNetwork(down_sample_count=1).to(config.DEVICE)]\n",
    "generator = GenerativeNetwork().to(config.DEVICE)\n",
    "\n",
    "generator.load_state_dict(torch.load(\"saved_models/generator_state_dict.pt\"))\n",
    "generator.to(config.DEVICE)\n",
    "for i in range(len(discriminators)):\n",
    "    discriminators[i].load_state_dict(torch.load(f\"saved_models/discriminator{i}_state_dict.pt\"))\n",
    "    discriminators[i].to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5020683a-557c-42e7-a2fa-b50b59620fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(generator, discriminators):\n",
    "    d_optimizers = []\n",
    "    for i in range(len(discriminators)):\n",
    "        d_optimizers.append(optim.Adam(discriminators[i].parameters(),\n",
    "                                       lr=config.LR,\n",
    "                                       betas=(0.5, 0.999)))\n",
    "    g_optimizer = optim.Adam(generator.parameters(),\n",
    "                             lr=config.LR,\n",
    "                            betas=(0.5, 0.999))\n",
    "    return g_optimizer, d_optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec535499-efb6-4e48-903b-0f9f222e32b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "data_loader = DataLoader(data_set, batch_size=2, shuffle=True)\n",
    "\n",
    "para = list(rnn_model.parameters())\n",
    "for p in cnn_model.parameters():\n",
    "    if p.requires_grad:\n",
    "        para.append(p)\n",
    "\n",
    "config.RNN_GRAD = 0.25\n",
    "\n",
    "print(data_loader.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33f08dc1-01b7-43aa-bc5d-943995f2ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(netD, real_imgs, fake_imgs, conditions,\n",
    "                       real_labels, fake_labels):\n",
    "    # Forward\n",
    "    real_features = netD(real_imgs)\n",
    "    fake_features = netD(fake_imgs.detach())\n",
    "    # loss\n",
    "    #\n",
    "    cond_real_logits = netD.conditional_discriminator(real_features, conditions)\n",
    "    cond_real_errD = nn.BCELoss()(cond_real_logits, real_labels)\n",
    "    cond_fake_logits = netD.conditional_discriminator(fake_features, conditions)\n",
    "    cond_fake_errD = nn.BCELoss()(cond_fake_logits, fake_labels)\n",
    "    #\n",
    "    batch_size = real_features.size(0)\n",
    "    cond_wrong_logits = netD.conditional_discriminator(real_features[:(batch_size - 1)], conditions[1:batch_size])\n",
    "    cond_wrong_errD = nn.BCELoss()(cond_wrong_logits, fake_labels[1:batch_size])\n",
    "\n",
    "    if netD.unconditional_discriminator is not None:\n",
    "        real_logits = netD.unconditional_discriminator(real_features)\n",
    "        fake_logits = netD.unconditional_discriminator(fake_features)\n",
    "        real_errD = nn.BCELoss()(real_logits, real_labels)\n",
    "        fake_errD = nn.BCELoss()(fake_logits, fake_labels)\n",
    "        errD = ((real_errD + cond_real_errD) / 2. +\n",
    "                (fake_errD + cond_fake_errD + cond_wrong_errD) / 3.)\n",
    "    else:\n",
    "        errD = cond_real_errD + (cond_fake_errD + cond_wrong_errD) / 2.\n",
    "    return errD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fb9175c-b738-4cb5-95b3-9b9813fed821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(netsD, image_encoder, fake_imgs, real_labels,\n",
    "                   words_embs, sent_emb, match_labels,\n",
    "                   cap_lens, class_ids):\n",
    "    numDs = len(netsD)\n",
    "    batch_size = real_labels.size(0)\n",
    "    \n",
    "    logs = ''\n",
    "    # Forward\n",
    "    errG_total = 0\n",
    "    for i in range(numDs):\n",
    "        features = netsD[i](fake_imgs[i])\n",
    "        cond_logits = netsD[i].conditional_discriminator(features, sent_emb)\n",
    "        cond_errG = nn.BCELoss()(cond_logits, real_labels)\n",
    "        if netsD[i].unconditional_discriminator is  not None:\n",
    "            logits = netsD[i].unconditional_discriminator(features)\n",
    "            errG = nn.BCELoss()(logits, real_labels)\n",
    "            g_loss = errG + cond_errG\n",
    "        else:\n",
    "            g_loss = cond_errG\n",
    "        errG_total += g_loss\n",
    "        # err_img = errG_total.data[0]\n",
    "        \n",
    "        # Ranking loss\n",
    "        if i == (numDs - 1):\n",
    "            # words_features: batch_size x nef x 17 x 17\n",
    "            # sent_code: batch_size x nef\n",
    "            region_features, cnn_code = image_encoder(fake_imgs[i])\n",
    "            w_loss0, w_loss1, _ = word_loss(region_features, words_embs,\n",
    "                                             cap_lens, match_labels,\n",
    "                                             class_ids, batch_size)\n",
    "            w_loss = (w_loss0 + w_loss1) * config.LAMBDA\n",
    "            # err_words = err_words + w_loss.data[0]\n",
    "\n",
    "            s_loss0, s_loss1 = sent_loss(cnn_code, sent_emb,\n",
    "                                         match_labels, class_ids, batch_size)\n",
    "            s_loss = (s_loss0 + s_loss1) * config.LAMBDA\n",
    "            # err_sent = err_sent + s_loss.data[0]\n",
    "\n",
    "            errG_total += w_loss + s_loss\n",
    "    return errG_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf7fb5bc-1a48-4d80-98b8-425869abef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "def save_generate_img(generator, epoch, statics):\n",
    "    fake_imgs, _, _, _ = generator(*statics)\n",
    "    imgx64 = fake_imgs[0]\n",
    "    imgx128 = fake_imgs[1]\n",
    "\n",
    "    save_image(imgx64[0], f\"x64/{epoch}.png\")\n",
    "    save_image(imgx128[0], f\"x128/{epoch}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "256adda8-c931-4ca1-950c-8a1a3575a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cnn_model, rnn_model, generator, discriminators, data_loader, damsm_train=False):\n",
    "    generator.train()\n",
    "    for i in range(len(discriminators)):\n",
    "        discriminators[i].train()\n",
    "\n",
    "    g_optim, d_optims = get_optimizer(generator, discriminators)\n",
    "    rc_optimizer = get_rc_optimizer(rnn_model, cnn_model)\n",
    "\n",
    "    real_labels = torch.ones((data_loader.batch_size,)).float().to(config.DEVICE)\n",
    "    fake_labels = torch.zeros((data_loader.batch_size,)).float().to(config.DEVICE)\n",
    "    labels = torch.LongTensor(list(range(data_loader.batch_size))).to(config.DEVICE)\n",
    "    z_noise = config.Z_DIM\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), config.RNN_GRAD)\n",
    "    \n",
    "    statics = None\n",
    "\n",
    "    for epoch in range(1):\n",
    "        cur_time = time.time()\n",
    "        for idx, (imgs, caps, lengths, ids) in enumerate(data_loader):\n",
    "            batch_size = imgs.size(0)\n",
    "            if batch_size < data_loader.batch_size: continue\n",
    "\n",
    "            imgs, caps, lengths, ids = imgs.to(config.DEVICE), caps.to(config.DEVICE), lengths.to(config.DEVICE), ids.to(config.DEVICE)\n",
    "\n",
    "            noises = torch.FloatTensor(batch_size, z_noise).to(config.DEVICE)\n",
    "\n",
    "            hidden = rnn_model.init_hidden(batch_size)\n",
    "\n",
    "            word_embs, sentences_emb = rnn_model(caps.long(), lengths.long(), hidden)\n",
    "\n",
    "            if damsm_train:\n",
    "                features, cnn_code = cnn_model(imgs)\n",
    "                w_loss0, w_loss1, _ = word_loss(features, word_embs, lengths, labels, ids, batch_size)\n",
    "                s_loss0, s_loss1 = sent_loss(cnn_code, sentences_emb, labels, ids, batch_size)\n",
    "\n",
    "                loss = w_loss0 + w_loss1 + s_loss0 + s_loss1\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                rc_optimizer.step()\n",
    "\n",
    "            word_embs, sentences_emb = word_embs.detach(), sentences_emb.detach()\n",
    "\n",
    "            mask = (caps == 0).to(config.DEVICE)\n",
    "            num_words = word_embs.size(2)\n",
    "\n",
    "            if mask.size(1) > num_words:\n",
    "                mask = mask[:, :num_words]\n",
    "\n",
    "            # create fakes image\n",
    "            noises.data.normal_(0, 1)\n",
    "            if statics is None:\n",
    "                statics = noises, sentences_emb, word_embs, mask\n",
    "\n",
    "            imgs = [F.interpolate(imgs, 64), F.interpolate(imgs, 128)]\n",
    "\n",
    "            total_d_loss = 0\n",
    "            \n",
    "            for _ in range(config.DISCRIMINATOR_REPEAT):\n",
    "                for i in range(len(discriminators)):\n",
    "                    d_noises = torch.randn((batch_size, z_noise)).to(config.DEVICE)\n",
    "                    fake_imgs = generator(d_noises, sentences_emb, word_embs, mask)[0]\n",
    "                    discriminators[i].zero_grad()\n",
    "                    loss = discriminator_loss(discriminators[i], imgs[i], fake_imgs[i], sentences_emb, real_labels, fake_labels)\n",
    "                    loss.backward()\n",
    "                    d_optims[i].step()\n",
    "\n",
    "                    total_d_loss += loss \n",
    "                    \n",
    "            fake_imgs, _, mu, log_var = generator(noises, sentences_emb, word_embs, mask)\n",
    "            # update generator\n",
    "            generator.zero_grad()\n",
    "            total_g_loss = generator_loss(discriminators, cnn_model, fake_imgs, real_labels,\n",
    "                                          word_embs, sentences_emb, labels, lengths, ids)\n",
    "            total_g_loss += torch.mean(mu.pow_(2).add_(log_var.exp()).mul_(-1).add_(1).add_(log_var)).mul_(-0.5)\n",
    "            total_g_loss.backward()\n",
    "            g_optim.step()\n",
    "            # if idx % 100 == 0:\n",
    "            #     print(idx, end=' ')\n",
    "        save_generate_img(generator, epoch, statics)\n",
    "        print(f\"Epoch {epoch} time {time.time() - cur_time}\")\n",
    "        print(f\"g_loss: {total_g_loss}, d_loss: {total_d_loss}\")\n",
    "\n",
    "        if damsm_train:\n",
    "            save_models(rnn_model, cnn_model, (\"rnn_model_state_dict.pt\", \"cnn_model_state_dict.pt\"))\n",
    "    return generator, discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e78748a-bdbe-4d2c-86d4-66241c7ca4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwan/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 1.94 GiB of which 21.19 MiB is free. Including non-PyTorch memory, this process has 1.60 GiB memory in use. Of the allocated memory 1.41 GiB is allocated by PyTorch, and 113.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdamsm_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 78\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(cnn_model, rnn_model, generator, discriminators, data_loader, damsm_train)\u001b[0m\n\u001b[1;32m     76\u001b[0m     total_g_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(mu\u001b[38;5;241m.\u001b[39mpow_(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(log_var\u001b[38;5;241m.\u001b[39mexp())\u001b[38;5;241m.\u001b[39mmul_(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(log_var))\u001b[38;5;241m.\u001b[39mmul_(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     77\u001b[0m     total_g_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 78\u001b[0m     \u001b[43mg_optim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# if idx % 100 == 0:\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m#     print(idx, end=' ')\u001b[39;00m\n\u001b[1;32m     81\u001b[0m save_generate_img(generator, epoch, statics)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    154\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:111\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    105\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    106\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros((), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    113\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 1.94 GiB of which 21.19 MiB is free. Including non-PyTorch memory, this process has 1.60 GiB memory in use. Of the allocated memory 1.41 GiB is allocated by PyTorch, and 113.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train(cnn_model, rnn_model, generator, discriminators, data_loader, damsm_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140f59b-9bc7-44a4-9e2b-066bc6487904",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), \"saved_models/generator_state_dict.pt\")\n",
    "torch.save(discriminators[0].state_dict(), \"saved_models/discriminator0_state_dict.pt\")\n",
    "torch.save(discriminators[1].state_dict(), \"saved_models/discriminator1_state_dict.pt\")\n",
    "\n",
    "torch.save(rnn_model.state_dict(), \"saved_models/rnn_model_state_dict.pt\")\n",
    "torch.save(cnn_model.state_dict(), \"saved_models/cnn_model_state_dict.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
